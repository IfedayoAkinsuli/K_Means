{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2: k-means clustering\n",
    "In this coursework, you will implement and test an unsupervised machine learning algorithm: _k-means clustering_. Once you have completed your work, you will need to submit the results on QM+. To submit your work, first download/export your Jupyter notebook as PDF. Then upload the PDF file in the submission area on QM+."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "As first step, familiarise yourself with k-means clustering, for example by working through https://en.wikipedia.org/wiki/K-means_clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Prepare two test datasets\n",
    "Pick a small set of values that enable you to apply clustering using pen&paper. Next create a fresh, or find an existing, large dataset. You may choose to use the Iris dataset, which is included in scikit-learn (and also available as a CSV file on QM+, generated using the below code snippet).\n",
    "> Marking information: Up to 10 points: clarify why you believe your small dataset to be suitable for k-means clustering. Also make sure you report the source of your large dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "# not necessary on EECS Jupyterlab systems\n",
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.8/site-packages')\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "print(iris.DESCR)\n",
    "\n",
    "import csv\n",
    "with open('iris.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    writer.writerow(iris.feature_names)\n",
    "    writer.writerows(iris.data.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Apply k-means clustering on paper\n",
    "Work through your small dataset from Task 1 to ensure you have known-good results.\n",
    "> Marking information: Up to 20 points: include information on how you sanity-checked your results as the number of iterations may considerably affect the precision of your final values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Create a test harness\n",
    "Start preparing your implementation by first setting up a test (without having an implementation just yet!). In this way, you will follow a _test-driven development_ approach. As part of this work, you may choose to compare to a reference implementation, like the one shown in the below code example.\n",
    "> Marking information: Up to 20 points: identify suitable unit tests and integration tests. Explain what coverage you expect your test suite to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2\n",
      " 2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 1 2 2 2 2 1 2 2 2 1 2 2 2 1 2\n",
      " 2 1]\n",
      "[[5.006      3.428      1.462      0.246     ]\n",
      " [5.9016129  2.7483871  4.39354839 1.43387097]\n",
      " [6.85       3.07368421 5.74210526 2.07105263]]\n"
     ]
    }
   ],
   "source": [
    "# see https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html for the full API\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(iris.data)\n",
    "\n",
    "print(kmeans.labels_)\n",
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Implement k-means clustering in Python\n",
    "You are now set to actually add the implementation. Note that you are expected to fully implement the mathematical operations instead of using a library function such as `scikit` or `statsmodels`. Your implementation may make several assumptions about the inputs provided to it. Make those explicit in comments. Also, provide an estimate on how long execution of your algorithm will take dependent on the input values. Express this estimate as a function of the inputs, such as the number of clusters and/or the number of data points.\n",
    "> Marking information: Up to 50 points: 30 points for a correctly working Python implementation, 10 points for describing limitations and assumptions of your implementation, and 10 points for a description of the complexity of your algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Task 5: Add a visualisation\n",
    "Identify a suitable library to depict both your inputs as well as the results of k-means clustering applied to those inputs.\n",
    "> Marking information: Up to 20 _bonus_ points: if you choose to complete this _optional_ task, you may be able to recover marks lost elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def points_picker(k,points):\n",
    "#     n = points.index\n",
    "#     from itertools import combinations\n",
    "#     combi = list(combinations(n,r=k))\n",
    "#     return combi\n",
    "#\n",
    "# def clusterer(sample,centroids,k:int,initial_len):\n",
    "#     from scipy.spatial.distance import euclidean\n",
    "#     cluster = []\n",
    "#\n",
    "#     for j in range(0,len(sample)):\n",
    "#         smallest_distance = {i:euclidean(u=sample.iloc[j,0:initial_len],v=centroids.iloc[i,0:initial_len]) for i in range(0,k)}\n",
    "#         cluster.append(min(smallest_distance,key=smallest_distance.get))\n",
    "#\n",
    "#     sample = sample.assign(Cluster = cluster)\n",
    "#     return sample\n",
    "#\n",
    "# def Kmeans(k:int,sample,no_iterations:int):\n",
    "#     from scipy.spatial.distance import euclidean\n",
    "#     test = points_picker(k=k,points=sample)\n",
    "#\n",
    "#     initial_len = sample.shape[1]\n",
    "#\n",
    "#     for m in range(0,len(test),int(len(test)/2)):\n",
    "#         min_sse = 1000\n",
    "#         SSE = []\n",
    "#         centroids = sample.iloc[list(test[m])]\n",
    "#\n",
    "#         for i in range(0,no_iterations):\n",
    "#             sample = clusterer(sample,centroids,k,initial_len)\n",
    "#             centroids = sample.groupby('Cluster').mean()\n",
    "#\n",
    "#         for j in range(0,k):\n",
    "#             error = [(euclidean(u=sample.loc[sample.Cluster == j].reset_index(drop=True).drop(['Cluster'],axis=1).iloc[i],v=centroids.iloc[j]))**2 for i in range(0,len(sample.loc[sample.Cluster == j]))]\n",
    "#             error = sum(error)\n",
    "#             SSE.append(error)\n",
    "#\n",
    "#         SSE = round(sum(SSE)/len(SSE),4)\n",
    "#\n",
    "#         if SSE<min_sse:\n",
    "#             min_sse=SSE\n",
    "#             solution=sample\n",
    "#             solution_centroids = centroids\n",
    "#\n",
    "#     return solution,min_sse,solution_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def clusterer(sample,centroids,k:int,initial_len):\n",
    "    from scipy.spatial.distance import euclidean\n",
    "    cluster = []\n",
    "\n",
    "    for j in range(0,len(sample)):\n",
    "        smallest_distance = {i:euclidean(u=sample.iloc[j,0:initial_len],v=centroids.iloc[i,0:initial_len]) for i in range(0,len(centroids))}\n",
    "        cluster.append(min(smallest_distance,key=smallest_distance.get))\n",
    "\n",
    "    sample = sample.assign(Cluster = cluster)\n",
    "    return sample\n",
    "\n",
    "def points_picker(k,points):\n",
    "    n = points.index\n",
    "    from itertools import combinations\n",
    "    combi = list(combinations(n,r=k))\n",
    "    return combi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x_coordinates = [1, 3.5, 2.3, 1.5, 0.75, 6.3, 7.8, 8.2, 9, 9.9, 5, 5.5, 6.9, 8.2, 9.7, 2, 3, 4, 5, 0.5]\n",
    "\n",
    "y_coordinates = [4, 3, 3.7, 3.9, 4.2, 0.4, 0.25, 1, 1.3, 2, 4, 3, 4.5, 5, 4.9, 7, 8, 9, 10, 8.4]\n",
    "\n",
    "sample = pd.DataFrame({'x_coordinate': x_coordinates, 'y_coordinate': y_coordinates})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "k=4\n",
    "test = points_picker(k=k,points=sample)\n",
    "\n",
    "initial_len = sample.shape[1]\n",
    "\n",
    "for m in range(0,len(test),int(len(test)/8)):\n",
    "    min_sse = 1000\n",
    "    SSE = []\n",
    "    centroids = sample.iloc[list(test[m])]\n",
    "\n",
    "    for i in range(0,5):\n",
    "        sample = clusterer(sample,centroids,k,initial_len)\n",
    "        centroids = sample.groupby('Cluster').mean()\n",
    "\n",
    "    for j in range(0,k):\n",
    "        error = [(euclidean(u=sample.loc[sample.Cluster == j].reset_index(drop=True).drop(['Cluster'],axis=1).iloc[i],v=centroids.iloc[j]))**2 for i in range(0,len(sample.loc[sample.Cluster == j]))]\n",
    "        error = sum(error)\n",
    "        SSE.append(error)\n",
    "\n",
    "    SSE = round(sum(SSE)/len(SSE),4)\n",
    "\n",
    "    if SSE<min_sse:\n",
    "        min_sse=SSE\n",
    "        solution=sample\n",
    "        solution_centroids = centroids\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
